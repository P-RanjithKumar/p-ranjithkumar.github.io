<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Blog 3 | Ranjith Kumar</title>
  <style>
    /* Reuse existing styles from homepage */
    :root {
      --primary-color: #2d3436;
      --accent-color: #0984e3;
      --text-color: #2d3436;
      --hover-color: #74b9ff;
    }
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }
    body {
      font-family: 'Segoe UI', system-ui, sans-serif;
      line-height: 1.6;
      color: var(--text-color);
      margin: 0 auto;
      padding: 20px;
      max-width: 1200px;
    }

    /* Fixed Scroll Progress Indicator */
    .scroll-progress {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      height: 4px;
      background: #eee;
      z-index: 1000;
    }
    .progress-bar {
      width: 0%;
      height: 100%;
      background: var(--accent-color);
    }

    /* Post Header */
    .post-header {
      margin-bottom: 3rem;
      position: relative;
      padding-top: 10px;
    }
    .back-to-blog {
      display: inline-flex;
      align-items: center;
      gap: 0.5rem;
      text-decoration: none;
      color: var(--accent-color);
      margin-bottom: 2rem;
      transition: color 0.2s ease;
    }
    .back-to-blog:hover {
      color: var(--hover-color);
    }
    .post-meta {
      display: flex;
      gap: 15px;
      color: #636e72;
      font-size: 0.9em;
      margin: 1rem 0;
    }

    /* Post Content */
    .post-content {
      display: grid;
      grid-template-columns: 1fr 250px;
      gap: 2rem;
    }
    .article-body {
      max-width: 800px;
      font-size: 1.1em;
      line-height: 1.8;
    }
    .article-body img {
      width: 100%;
      height: auto;
      border-radius: 8px;
      margin: 2rem 0;
    }
    .article-body h2,
    .article-body h3 {
      margin: 2rem 0 1rem;
      color: var(--primary-color);
    }
    blockquote {
      border-left: 4px solid var(--accent-color);
      padding: 1rem 2rem;
      margin: 2rem 0;
      background: #f8f9fa;
      border-radius: 0 8px 8px 0;
    }

    /* Table of Contents */
    .toc {
      position: sticky;
      top: 20px;
      align-self: start;
      padding: 1.5rem;
      background: #f8f9fa;
      border-radius: 8px;
    }
    .toc ul {
      list-style: none;
      padding-left: 1rem;
    }
    .toc a {
      color: var(--text-color);
      text-decoration: none;
      transition: color 0.2s ease;
      display: block;
      padding: 0.3rem 0;
    }
    .toc a:hover {
      color: var(--accent-color);
    }

    /* Post Navigation */
    .post-navigation {
      margin: 4rem 0;
      display: flex;
      justify-content: space-between;
      border-top: 1px solid #eee;
      padding-top: 2rem;
    }
    .nav-button {
      padding: 12px 25px;
      text-decoration: none;
      border-radius: 25px;
      transition: all 0.2s ease;
    }
    .prev-post {
      background: var(--accent-color);
      color: white;
    }
    .next-post {
      background: #f1f2f6;
      color: var(--text-color);
    }
    .nav-button:hover {
      transform: translateY(-2px);
    }

    .connect-container {
      max-width: 800px;
      margin: 2rem auto;
      padding: 2rem;
      border-radius: 10px;
      background: linear-gradient(to right, #f8f9fa, #e9ecef);
      box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    }
    
    .connect-heading {
      color: #2d3748;
      font-size: 2rem;
      margin-bottom: 1rem;
      position: relative;
      padding-bottom: 0.5rem;
    }
    
    .connect-heading::after {
      content: "";
      position: absolute;
      left: 0;
      bottom: 0;
      height: 3px;
      width: 60px;
      background-color: #4299e1;
      border-radius: 3px;
    }
    
    .connect-text {
      color: #4a5568;
      font-size: 1.1rem;
      line-height: 1.6;
      margin-bottom: 1.5rem;
    }
    
    .social-links {
      display: flex;
      flex-wrap: wrap;
      gap: 1rem;
      list-style: none;
      padding: 0;
    }
    
    .social-link {
      display: flex;
      align-items: center;
      padding: 0.75rem 1.5rem;
      background-color: white;
      border-radius: 50px;
      text-decoration: none;
      color: #2d3748;
      font-weight: 500;
      transition: all 0.3s ease;
      box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
    }
    
    .social-link:hover {
      transform: translateY(-3px);
      box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
    }
    
    .social-link:hover .icon {
      transform: rotate(10deg);
    }
    
    .linkedin:hover {
      background-color: #0077b5;
      color: white;
    }
    
    .instagram:hover {
      background: linear-gradient(45deg, #f09433, #e6683c, #dc2743, #cc2366, #bc1888);
      color: white;
    }
    
    .github:hover {
      background-color: #333;
      color: white;
    }
    
    .icon {
      margin-right: 10px;
      transition: transform 0.3s ease;
    }
    
    @media (max-width: 600px) {
      .social-links {
        flex-direction: column;
      }
      
      .social-link {
        width: 100%;
        justify-content: center;
      }
    }

    /* New Math Section Styles */
    .math-section {
      font-family: 'Segoe UI', system-ui, sans-serif;
    }
    
    .math-card {
      background: linear-gradient(145deg, #ffffff, #f5f7fa);
      border-radius: 12px;
      padding: 24px;
      margin: 20px 0;
      box-shadow: 0 4px 15px rgba(0, 0, 0, 0.05);
    }
    
    .math-formula {
      background: #fff;
      padding: 20px;
      border-radius: 8px;
      border-left: 4px solid #0984e3;
      margin: 15px 0;
      font-size: 1.1em;
    }
    
    .math-breakdown {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
      gap: 20px;
      margin: 20px 0;
    }
    
    .math-component {
      background: #f8fafc;
      padding: 15px;
      border-radius: 8px;
      transition: transform 0.2s ease;
    }
    
    .math-component:hover {
      transform: translateY(-2px);
    }
    
    .math-title {
      color: #0984e3;
      font-weight: 600;
      margin-bottom: 8px;
    }

    /* Responsive Design */
    @media (max-width: 768px) {
      .post-content {
        grid-template-columns: 1fr;
      }
      .toc {
        position: static;
        margin-bottom: 2rem;
      }
      .post-navigation {
        flex-direction: column;
        gap: 1rem;
      }
      .math-breakdown {
        grid-template-columns: 1fr;
      }
    }
  </style>
</head>
<body>
  <!-- Scroll Progress Indicator -->
  <div class="scroll-progress">
    <div class="progress-bar"></div>
  </div>

  <header class="post-header">
    <a href="bloghome.html" class="back-to-blog">← Back to Blog</a>
    <h1>How I learnt Deep Learning and LLMs: A Tale of Code, Curiosity, and Andrej Karpathy</h1>
    <div class="post-meta">
      <span>Feb 23, 2025</span>
      <span>•</span>
      <span>10 min read</span>
    </div>
    <div class="post-tags">
      <span class="tag">Deep Learning  </span>
      <span class="tag">Blogging</span>
    </div>
  </header>
  
  <div class="post-content">
    <article class="article-body">
      <!-- Cover Image -->
      <img src="assets/blog_cover_image/neuralnetwork.png" alt="Blog post cover image" style="border-radius: 8px; width: 800px; height: 150px;">
  
      <!-- Introduction -->
      <p>Hey there! I’m Ranjith Kumar, and today I’m taking you on a journey—from my first steps in deep learning to wrestling with large language models (LLMs) and even dipping my toes into reinforcement learning (RL). It’s been a wild ride of late-night coding, GPU rentals, and a YouTube obsession that changed everything. If you’re curious about how LLMs work, what makes attention mechanisms tick, or why RL is suddenly my new jam, you’re in the right place. Let’s break it all down, step by step, with plenty of visuals and beginner-friendly explanations along the way.</p>
  
      <!-- Section 1 -->
      <h2 id="section1">1. The Spark: PyTorch, Python, and a Hunger to Learn</h2>
      <p>Like many of you, my deep learning journey started with the basics: Python and PyTorch. In college, I learned just enough to build simple neural networks—but let’s be honest, I was the kid tweaking code and praying it wouldn’t crash. The real magic happened outside class, down the YouTube rabbit hole. I devoured tutorials on LLMs, trying to understand how models like GPT churn out human-like text. Most videos were decent, but they left me with more questions than answers. That is, until I found my deep learning hero: Andrej Karpathy.</p>
  
      <!-- Section 2 -->
      <h2 id="section2">2. Andrej Karpathy: The Teacher I Never Met</h2>
      <p>One day, I stumbled upon Andrej Karpathy’s video on GPT-2, and it was like the clouds parted. His chill, no-nonsense style made even the toughest concepts feel approachable. I became a fanboy overnight, binging everything he’d ever posted. Thanks to Andrej, I finally cracked two of the trickiest parts of LLMs: <strong>attention mechanisms</strong> and <strong>tokenization</strong>. If you’re new to these terms, don’t worry—we’re about to break them down together.</p>
      <!-- Section 3 -->
      <h2 id="section3">3. Attention Mechanisms: The Secret Sauce of LLMs</h2>
      <p>Let's start with attention mechanisms. Imagine you're reading this sentence: "The cat slept while the dog barked." Your brain naturally focuses on "cat" and "slept" to understand what's happening, right? Attention does the same for LLMs—it helps the model focus on the most important parts of the input when predicting the next word.</p>
      
      <!--  Math Section -->
      <div class="math-section">
        <div class="math-card">
          <p>The attention mechanism can be broken down into a simple formula that shows how different parts work together:</p>
          
          <div class="math-formula">
            <center>
              Attention(Q, K, V) = softmax(QK<sup>T</sup> / √d<sub>k</sub>) × V
            </center>
          </div>
          
          <div class="math-breakdown">
            <div class="math-component">
              <div class="math-title">Step 1: Similarity Score</div>
              <p>QK<sup>T</sup> measures how well each word matches with others - like finding connections between words.</p>
            </div>
            
            <div class="math-component">
              <div class="math-title">Step 2: Scaling (√d<sub>k</sub>)</div>
              <p>We divide by √d<sub>k</sub> to keep numbers manageable - think of it as turning down the volume when it's too loud.</p>
            </div>
            
            <div class="math-component">
              <div class="math-title">Step 3: Softmax</div>
              <p>Converts scores into percentages (0-100%) showing how much attention each word should get.</p>
            </div>
            
            <div class="math-component">
              <div class="math-title">Step 4: Final Output</div>
              <p>Multiply by V to get the weighted result - like mixing ingredients based on their importance in a recipe.</p>
            </div>
          </div>
        </div>
        
        <div class="math-card">
          <h4>💡 Quick Example</h4>
          <p>Imagine processing the sentence "The cat chased the mouse":</p>
          <ul>
            <li>When focusing on "cat", the model pays more attention to "chased" (action)</li>
            <li>When focusing on "chased", it pays attention to both "cat" (subject) and "mouse" (object)</li>
            <li>This helps the model understand who did what to whom</li>
          </ul>
        </div>
      </div>

      <!-- Section 4 -->
      <h2 id="section4">4. Tokenization: Turning Words into Numbers</h2>
      <p>Before attention can do its thing, the model needs to “read” the text. That’s where tokenization comes in. Tokenization chops text into smaller pieces called tokens—think of them as the model’s vocabulary. For example:</p>
      <ul>
        <li>“I love AI” might become [“I”, “love”, “AI”].</li>
        <li>Trickier words like “playing” could split into [“play”, “ing”] using subword tokenizers like BPE (Byte Pair Encoding).</li>
      </ul>
      <p>Each token gets a unique ID, and the model learns patterns from these IDs. Why’s this cool? It lets the model handle rare or new words by breaking them into familiar chunks. Imagine “unbelievable” as [“un”, “believ”, “able”]—even if it’s never seen the full word, it can guess the meaning. Tokenization is the first step in turning messy human language into something a machine can process.</p>
      <img src="assets/content_images/tokenization.png" alt="tokenization image" style="width: 700px; height: 400px;">

      <!-- Section 5 -->
      <h2 id="section5">5. My First LLM: From Gibberish to “Almost Chatbot”</h2>
      <p>Armed with Andrej’s teachings, I decided to build my own LLM. I rented a GPU (because my laptop would’ve cried), slapped together a transformer in PyTorch, and got to work. I coded up multi-head attention—where the model runs attention multiple times in parallel to capture different relationships—and added positional encodings (more on those later). After training on a small dataset, my first output was… well, gibberish. Think “cat the the dog umm.” But with some tweaks to the learning rate and more data, it started forming actual sentences. Not chatbot-level, but I was stoked.</p>
      <p>Andrej mentioned in one video that to make an LLM chatty, you need to fine-tune it on conversational data. I haven’t gotten there yet, but it’s on my to-do list. For now, I’m just happy I didn’t break anything.</p>
  
      <!-- Section 6 -->
      <h2 id="section6">6. The RL Detour: DeepSeek R1 and a Whole New World</h2>
      <p>Just when I thought I was getting comfy with LLMs, I stumbled across DeepSeek R1, a reasoning model that uses GRPO (Generalized Reward Policy Optimization). Cue the confusion—RL? What’s that? I’d barely scratched the surface, but suddenly I was diving headfirst into reinforcement learning. GRPO led me to PPO (Proximal Policy Optimization), the HER paper (Hindsight Experience Replay), curiosity learning, and more. RL was a beast, but I couldn’t look away.</p>
      <h3>Reinforcement Learning: Teaching Machines to Learn by Doing</h3>
      <p>Let’s break down RL for beginners. Imagine teaching a dog to fetch: you reward it with treats when it brings the ball and ignore it when it doesn’t. Over time, the dog learns that fetching = treats. RL works similarly: an agent takes actions in an environment, gets rewards (or penalties), and learns to maximize its total reward.</p>
      <p>Here are the core pieces:</p>
      <ul>
        <li><strong>Agent</strong>: The learner (e.g., a model or robot).</li>
        <li><strong>Environment</strong>: The world the agent interacts with.</li>
        <li><strong>Policy (π)</strong>: The strategy the agent uses to pick actions—like “if I’m here, do this.”</li>
        <li><strong>Reward Function</strong>: Defines what’s good or bad (e.g., +1 for fetching, -1 for ignoring).</li>
        <li><strong>Value Function</strong>: Estimates how good a state is in the long run.</li>
      </ul>
      <p>Unlike supervised learning, where you have labeled data, RL is all about trial and error. The agent explores, fails, and learns—like me trying to bake without a recipe. It’s slow and messy, but when it works, you’ve got a model that can play games, optimize schedules, or even drive cars.</p>
  
      <!-- Section 7 -->
      <h2 id="section7">7. Transformers: The Engine Behind LLMs</h2>
      <p>Now, let’s zoom back to LLMs and unpack the transformer architecture—the real MVP. Born from the “Attention is All You Need” paper, transformers ditched recurrent networks (RNNs) and went all-in on attention. Here’s how they work:</p>
      <ul>
        <li><strong>Encoder</strong>: Takes the input (tokenized text), runs it through multiple layers of attention and feed-forward networks, and produces rich embeddings that capture the meaning of each token in context.</li>
        <li><strong>Decoder</strong>: Uses the encoder’s output plus its own attention to generate text, one token at a time. It’s autoregressive, meaning it predicts the next word based on the previous ones.</li>
        <li><strong>Multi-Head Attention</strong>: Runs attention multiple times in parallel, each “head” focusing on different aspects (like grammar or meaning).</li>
        <li><strong>Positional Encodings</strong>: Since transformers don’t process sequentially, we add sine and cosine waves to the token embeddings to encode word order. Without this, “cat chased dog” and “dog chased cat” would look the same to the model!</li>
      </ul>
      <img src="assets/content_images/transformers.png" alt="transformer image">
      <p>Why are transformers so powerful? They process everything in parallel, making them fast, and attention lets them handle long-range dependencies. It’s why my baby LLM could eventually string sentences together, even if it’s not quite ready for prime time.</p>
  
      <!-- Section 8 -->
      <h2 id="section8">8. What’s Next? Fine-Tuning and RL Domination</h2>
      <p>I’m not done yet. My LLM needs fine-tuning on conversational data to chat like a human instead of a weird robot poet. And RL? I’ve got policy optimization on deck—more algorithms, more math, more coffee. The journey’s just heating up, and I’m here for it.</p>
  
      <p>Whew, you made it! If you’re into deep learning, LLMs, or RL, let’s geek out together. What’s your go-to resource? Any RL tricks up your sleeve? Drop a comment or ping me on socials:</p>
      <!-- ######### socials ##########-->
      <div class="connect-container" id="section9">
        <h2 class="connect-heading">Let's Connect & Grow Together</h2>
        <p class="connect-text">I'm passionate about sharing knowledge and building a community of like-minded professionals. Connect with me on these platforms and let's learn and grow together!</p>
        
        <ul class="social-links">
          <li>
            <a href="https://www.linkedin.com/in/ranjith-kumar-b66180250" class="social-link linkedin" target="_blank">
              <svg class="icon" xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="currentColor">
                <path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"/>
              </svg>
              LinkedIn
            </a>
          </li>
          <li>
            <a href="https://www.instagram.com/ranjithh_56" class="social-link instagram" target="_blank">
              <svg class="icon" xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="currentColor">
                <path d="M12 2.163c3.204 0 3.584.012 4.85.07 3.252.148 4.771 1.691 4.919 4.919.058 1.265.069 1.645.069 4.849 0 3.205-.012 3.584-.069 4.849-.149 3.225-1.664 4.771-4.919 4.919-1.266.058-1.644.07-4.85.07-3.204 0-3.584-.012-4.849-.07-3.26-.149-4.771-1.699-4.919-4.92-.058-1.265-.07-1.644-.07-4.849 0-3.204.013-3.583.07-4.849.149-3.227 1.664-4.771 4.919-4.919 1.266-.057 1.645-.069 4.849-.069zm0-2.163c-3.259 0-3.667.014-4.947.072-4.358.2-6.78 2.618-6.98 6.98-.059 1.281-.073 1.689-.073 4.948 0 3.259.014 3.668.072 4.948.2 4.358 2.618 6.78 6.98 6.98 1.281.058 1.689.072 4.948.072 3.259 0 3.668-.014 4.948-.072 4.354-.2 6.782-2.618 6.979-6.98.059-1.28.073-1.689.073-4.948 0-3.259-.014-3.667-.072-4.947-.196-4.354-2.617-6.78-6.979-6.98-1.281-.059-1.69-.073-4.949-.073zm0 5.838c-3.403 0-6.162 2.759-6.162 6.162s2.759 6.163 6.162 6.163 6.162-2.759 6.162-6.163c0-3.403-2.759-6.162-6.162-6.162zm0 10.162c-2.209 0-4-1.79-4-4 0-2.209 1.791-4 4-4s4 1.791 4 4c0 2.21-1.791 4-4 4zm6.406-11.845c-.796 0-1.441.645-1.441 1.44s.645 1.44 1.441 1.44c.795 0 1.439-.645 1.439-1.44s-.644-1.44-1.439-1.44z"/>
              </svg>
              Instagram
            </a>
          </li>
          <li>
            <a href="https://github.com/P-RanjithKumar" class="social-link github" target="_blank">
              <svg class="icon" xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="currentColor">
                <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/>
              </svg>
              GitHub
            </a>
          </li>
        </ul>
      </div>
      <p>Keep coding, stay curious, and embrace the chaos—it’s where the good stuff happens!</p>
  
      <!-- Post Navigation -->
      <div class="post-navigation">
        <a href="blog2.html" class="nav-button prev-post">← Previous Post</a>
        <a href="blog4.html" class="nav-button next-post">Next Post →</a>
      </div>
    </article>
  
    <!-- Table of Contents -->
    <aside class="toc">
      <h3>Table of Contents</h3>
      <ul>
        <li><a href="#section1">The Spark: PyTorch, Python, and a Hunger to Learn</a></li>
        <li><a href="#section2">Andrej Karpathy: The Teacher I Never Met</a></li>
        <li><a href="#section3">Attention Mechanisms: The Secret Sauce of LLMs</a></li>
        <li><a href="#section4">Tokenization: Turning Words into Numbers</a></li>
        <li><a href="#section5">My First LLM: From Gibberish to "Almost Chatbot"</a></li>
        <li><a href="#section6">The RL Detour: DeepSeek R1 and a Whole New World</a></li>
        <li><a href="#section7">Transformers: The Engine Behind LLMs</a></li>
        <li><a href="#section8">What's Next? Fine-Tuning and RL Domination</a></li>
        <li><a href="#section9">Your Turn!</a></li>
      </ul>
    </aside>
  </div>

  <!--###############################################################################################################-->

  <script>
    // Scroll Progress Indicator Update
    window.addEventListener('scroll', () => {
      const winScroll = document.body.scrollTop || document.documentElement.scrollTop;
      const height = document.documentElement.scrollHeight - document.documentElement.clientHeight;
      const scrolled = (winScroll / height) * 100;
      document.querySelector('.progress-bar').style.width = scrolled + "%";
    });

    // Smooth Scroll for Table of Contents
    document.querySelectorAll('.toc a').forEach(anchor => {
      anchor.addEventListener('click', function(e) {
        e.preventDefault();
        const section = document.querySelector(this.getAttribute('href'));
        section.scrollIntoView({ behavior: 'smooth' });
      });
    });

    // Estimated Reading Time
    const postContent = document.querySelector('.article-body');
    const words = postContent.innerText.split(' ').length;
    const readingTime = Math.ceil(words / 200);
    document.querySelector('.post-meta span:nth-child(5)').innerHTML = 
      `${readingTime} min read`;
  </script>
</body>
</html>
