Blog 8: From Theory to Tensor â€“ The Reality of Building Causal AI
Date: November 20, 2025
Location: DCU, Dublin (probably raining outside, letâ€™s be honest)
Mood: Debugging Mode ðŸ›
Reading Time: ~25 Minutes

Introduction: The "Hello World" of Phase 2
Welcome back to the practicum diary.

If youâ€™ve been following along (or if youâ€™re just joining this chaotic journey now), you know the mission: We are trying to teach an AI to understand "Why" instead of just "What."

We are building a Hybrid LLMâ€”a Frankenstein monster that stitches together the linguistic fluency of a Transformer (BERT) with the logical rigor of a Structural Causal Model (SCM). The goal? To stop AI from thinking that yellow fingers cause lung cancer just because they often appear together. We want it to understand that smoking causes both.

In the last few blogs, we lived in the comfortable world of Theory. We wrote proposals, we read Judea Pearlâ€™s The Book of Why, and we designed beautiful diagrams of how our system should work.

Well, the honeymoon phase is over.
Over the last two weeks, we moved from Phase 1 (Foundation) to Phase 2 (Implementation). We stopped reading papers and started writing Python code. We loaded datasets, fired up GPUs, and ran our first experiments.

And guess what? It broke.
But thatâ€™s where the real story begins. This blog is about the gritty reality of turning a research proposal into running codeâ€”the data cleaning, the baseline failures, and the "Aha!" moments that only come at 2 AM.

Chapter 1: The Dataset Deep Dive (Balanced COPA)
Before we could build our "Causal Brain," we needed a test to prove it works. You canâ€™t just ask an AI, "Are you causal now?" You need a benchmark.

We chose COPA (Choice of Plausible Alternatives).
But not just any COPA. We are using Balanced COPA.

The Technical Stuff (Accurate & Careful)
Standard COPA is a dataset where the model is given a premise and two alternatives, and it must pick the more plausible cause or effect.

Example:

Premise: The man turned on the tap.

Question: What happened as a result?

Choice 1: Water flowed out. (Correct)

Choice 2: The sink broke. (Incorrect)

The problem with standard COPA is that LLMs are lazy. They cheat. They often pick the answer that just "sounds" more related, based on word frequency (correlations), without actually doing any causal reasoning.

Balanced COPA fixes this by introducing Mirrored Pairs.
For every question in the dataset, there is a "mirror" version where the correct answer is flipped or the context is changed slightly to neutralize simple keyword matching. This ensures that if the model gets the answer right, itâ€™s not because it memorized a pattern, but because it (hopefully) understood the logic.

The Practical Experience
Loading this wasn't just import dataset. We had to inspect the schema carefully.
In our recent coding sessions, we found that the column names were crucial. It wasnâ€™t just text and label. We had to map:

premise â†’ The starting event.

choice1 / choice2 â†’ The options.

question â†’ Are we looking for the cause or the effect?

mirrored â†’ A boolean flag (True/False) that tells us if this is a trick question designed to catch the AI cheating.

We spent a good chunk of time just writing the preprocessing function. In BERT, you can't just feed in three sentences. You have to format them like this:
[CLS] Premise [SEP] Choice 1 [SEP]
And then run a second pass for:
[CLS] Premise [SEP] Choice 2 [SEP]

This "siamese network" approach forces the model to score both options independently. Writing the code to handle these mirrored flags was our first technical win of the month.

Chapter 2: The Baseline (Setting the Bar)
Every good scientist needs a control group. Before we could inject our fancy Causal Graph, we needed to know how a "dumb" model performs.

We set up a BERT-base-uncased model. This is our "Correlation Machine." It doesnâ€™t know causality; it only knows statistics.
We fired up the GPU (cuda:0, naturally) and ran the training loop.

The Hypothesis:
We expected BERT to get around 60-70% accuracy. Why? Because BERT is smart, but itâ€™s easily fooled by those "mirrored" examples in Balanced COPA.

The Reality:
Getting the training loop to run was a lesson in PyTorch patience.

Device checks: ensuring the tensors were actually on the GPU.

Tokenization: We had to ensure our max length wasn't chopping off the important parts of the sentences.

The "Story" of the Loss Curve: Watching the training loss go down is the most satisfying thing in AI. It started high, dipped, and then... plateaued.

We established our baseline. This number is now the "Floor." Our Hybrid model must beat this number. If it doesn't, we have failed. (No pressure).

Chapter 3: The Hybrid Experiment (and the Failure)
Here is the meat of the update. This is where things got interestingâ€”and frustrating.

The Architecture
We designed a system with a tunable parameter: Alpha ($\alpha$).

$\alpha = 0.0$: Pure BERT (100% Correlation).

$\alpha = 1.0$: Pure Causal SCM (100% Logic).

$\alpha = 0.5$: The Hybrid (50% Stats + 50% Logic).

We theorized that as we increased Alpha from 0 to 0.5, the accuracy should go up. The SCM should act as a "Logic Police," catching BERT when it makes a stupid mistake.

The "48% Anomaly"
We ran the experiment. We waited. We checked the logs.
And we saw something strange.

For Alpha 0.0, 0.2, 0.4, 0.6, 0.8... the accuracy was identical. It was stuck at roughly 48%.
It was flat. The line wasn't moving.
Only when we hit Alpha 1.0 (Pure Causal) did the number jump/change (to about 50%).

The Diagnosis:
We sat there staring at the screen. Why is the Causal Module doing nothing?
Then it hit us (and we confirmed this in our chat history on Nov 10):
The Causal Graph was empty.

Or, more accurately, our Extraction Logic was failing.
We were trying to build the causal graph automatically using rule-based extraction (looking for words like "because", "so", "therefore"). But COPA sentences are often subtle. They don't always say "The glass broke because it fell." They say "The glass fell. It shattered."

Because our extractor missed the connection, the SCM graph had no edges.
If the graph has no edges, it provides zero information.
So, Hybrid Prediction = BERT + (0 * SCM).
No wonder the accuracy didn't change! We were essentially adding zero to our BERT score for every alpha value.

The Lesson
This was a classic "Garbage In, Garbage Out" moment. We were so focused on the integration (the Alpha parameter, the pipeline) that we neglected the input (the graph construction).
Technical takeaway: A Hybrid Neuro-Symbolic system is only as strong as its Symbolic component. If your symbols (the graph) are weak, your Neural network (BERT) will just ignore them.

Chapter 4: Side Quests (University Life)
While wrestling with empty graphs, the rest of the MSc didn't stop. We had to juggle the "Practicum" with the "Real World."

1. The Ethics Assignment (Samsung Case Study)
We took a detour into computing ethics. We analyzed a real-world case (Samsung) using Liffickâ€™s Analysis Method.
This was a refreshing break from coding. Instead of debugging tensors, we were debugging morality. We looked at the deontological ethicsâ€”the duty of a company to its users. It reminded me that what we are building (AI decision-making) has real consequences. If our Causal AI makes a decision in a medical context (like our lung cancer example), it needs to be right. Accuracy isn't just a scoreboard; it's an ethical obligation.

2. The Literature Review
We also finalized the formal Literature Review. We condensed 40+ papers into a coherent narrative. We looked at "LLMs for Causality" (using GPT to find causes) and "Causality for LLMs" (using causes to fix GPT).
Status: Done, dusted, and submitted (hopefully).

Chapter 5: The Road Ahead (Fixing the Graph)
So, where does that leave us?

We are currently in Debug Mode. The priority for the next sprint is clear: Better Causal Extraction.

We cannot rely on simple "keyword matching" to build our causal graphs. It's too brittle.
The New Plan:

Manual/Semi-Automatic Construction: For the purpose of this practicum, we might hand-craft the graphs for a subset of data to prove the concept works first.

Prompt-Based Extraction: We might use a larger LLM (like GPT-4 or Llama 3) offline to extract the causal triples (Cause -> Effect) from the text, build the graph, and then use that high-quality graph to guide our smaller BERT model.

The "Two-Step Pipeline":

Step 1: BERT makes a guess.

Step 2: SCM checks the guess against the graph.

Step 3: If they disagree, who wins? (This is where we need to tune our logic).

Conclusion
This month was a reality check.
We went from "We are going to solve Causal AI!" to "Why is my graph empty?"
But that is exactly what a practicum is supposed to be. If it worked the first time, it wouldn't be research; it would just be development.

We found the bottleneck (the graph extraction). We have the baseline (BERT). We have the dataset (Balanced COPA).
Now, we just need to bridge the gap.

Stay tuned for Blog 9, where hopefully, Iâ€™ll be sharing a chart where the line actually goes up.

Next milestone:

 Fix Causal Extraction logic.

 Run the Hybrid experiment again (and beat the 48% baseline).

 Survive the Dublin rain.

End of Log.

Author's Note for the Reader (Beginner Friendly)
LLM: Large Language Model (like ChatGPT or BERT). Good at talking, bad at logic.

SCM: Structural Causal Model. A graph of nodes and arrows that maps out logic (A causes B). Good at logic, can't talk.

Hybrid: Smashing them together so we get an AI that talks well and makes sense.

Epoch: One full run through the training data.

Alpha: A volume dial. Turn it left for "Pure Stats", turn it right for "Pure Logic".